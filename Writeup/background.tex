
\section{Regret Minimization}
\label{sec:background}

In order to evaluate performance, we must first assign the gain associated with results of playing a given move, known as the move's utility $U$.  We assign moves resulting in a win the utility of $+1$, a tie (both players chose the same action) $0$, and loss $-1$.

Consider a single move, where the opponent plays Rock and we play Scissors, with resulting utility 
$$U(\textrm{Scissors, Rock}) = -1$$  
Then we evaluate our regret to be the difference in utility between other moves we could have played and the move we opted for.  We evaluate our utility as
$$ r = U(\textrm{Rock, Rock}) - U(\textrm{Scissors, Rock}) = 0 - (-1) = 1$$
$$ r = U(\textrm{Paper, Rock}) - U(\textrm{Scissors, Rock}) = 1 - ( -1) = 2$$
It is important to note that the regret for a move that we \textbf{did} make is 0 -- choosing to play Scissors in that previous move would result in precisely the same outcome as it did when it was already played.  

So, for the single round, we have regrets $r = (1, 0, 2)$ (in rock, paper, scissor order).  To determine the next move, we probabilistically choose from each of the moves according to a mixed strategy derived from the normalized distribution of the regrets; that is, probabilistically from $(\frac{1}{3}, 0, \frac{2}{3})$.  If there are no positive regrets in a round, indicating that we played optimally, we opt for a random choice.

By aggregating the regrets and the mixed strategies over a large number of games in the recent history of the current opponent, we establish the opponent's current mixed strategy.  Note that this does not indicated an understanding of the opponent's algorithm, merely their performance over the most recent moves.  The result is a mixed strategy for our algorithm for the upcoming move.